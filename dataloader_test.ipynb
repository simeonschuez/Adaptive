{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "\n",
    "from data_utils import get_karpathy_split, refcoco_splits\n",
    "from data_loader import get_caption_loader, COCOCaptionDataset, get_reg_loader\n",
    "\n",
    "from build_vocab import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/simeon/Dokumente/Code/Uni/Repos/Adaptive/nlg-eval')\n",
    "from nlgeval import NLGEval\n",
    "nlgeval = NLGEval(no_skipthoughts=True, no_glove=True)  # loads the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size=224\n",
    "image_dir='/home/simeon/Dokumente/Code/Data/COCO/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/coco_vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df = get_karpathy_split(splits_path='/home/simeon/Dokumente/Code/Data/COCO/splits/karpathy/caption_datasets/', caps_path='/home/simeon/Dokumente/Code/Data/COCO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((crop_size, crop_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = caps_df.loc[caps_df.split == 'restval'].iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_caption_loader(\n",
    "        decoding_level='word', \n",
    "        split=['val'],\n",
    "        data_df=caps_df, \n",
    "        image_dir=image_dir, \n",
    "        vocab=vocab,\n",
    "        transform=transform, \n",
    "        batch_size=20, \n",
    "        shuffle=False,\n",
    "        num_workers=2, \n",
    "        drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, (images, captions, lengths, _, _) in enumerate(loader):    \n",
    "    if i > 2:\n",
    "        break\n",
    "    print(i)\n",
    "    \n",
    "idx = [i.item() for i in captions[2]]\n",
    "' '.join([vocab.idx2word[i] for i in idx])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "caps_df.loc[caps_df.image_id == 318556].caption.map(str.strip).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1251"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "hypotheses = []\n",
    "references = []\n",
    "\n",
    "for i, (images, _, _, image_ids, _) in enumerate(loader):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    # Build caption based on Vocabulary and the '<end>' token\n",
    "    for image_idx in range(images.size()[0]):\n",
    "\n",
    "        img_id = int(image_ids[image_idx])\n",
    "\n",
    "        refs = caps_df.loc[caps_df.image_id == img_id].caption.to_list()\n",
    "        references.append(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/refcoco_vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = refcoco_splits('/home/simeon/Dokumente/Code/Data/RefCOCO/refcoco/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_reg_loader(\n",
    "        decoding_level='word', \n",
    "        split=['val'],\n",
    "        data_df=ref_df.groupby('ann_id').agg('first').reset_index(), \n",
    "        image_dir=image_dir, \n",
    "        vocab=vocab,\n",
    "        transform=transform, \n",
    "        batch_size=20, \n",
    "        shuffle=False,\n",
    "        num_workers=2, \n",
    "        drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "hypotheses = []\n",
    "references = []\n",
    "\n",
    "for i, (images, targets, positions, lengths, ann_ids, filenames) in enumerate(loader):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    # Build caption based on Vocabulary and the '<end>' token\n",
    "    for ann_idx in range(images.size()[0]):\n",
    "\n",
    "        ann_id = int(ann_ids[ann_idx])\n",
    "\n",
    "        refs = ref_df.loc[ref_df.ann_id == ann_id].caption.to_list()\n",
    "        references.append(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i, (images, captions, positions, lengths, ann_ids, filenames) in enumerate(loader):    \n",
    "    if i > 2:\n",
    "        break\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> leftmost animal edge of pic <end> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [i.item() for i in captions[2]]\n",
    "' '.join([vocab.idx2word[i] for i in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the lady with the blue shirt', 'lady with back to us', 'blue shirt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = loader.dataset.df\n",
    "df.loc[df.ann_id == 1719310].caption.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
